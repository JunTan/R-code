
## Step 4: MODELING (Jun Tan, Dennis, Gong Ze)
```{r}
# load data
load("final_data_no_AK.Rda")
```

```{r}
# clean the given dataframe
# remove rows that have any of the column = NA removed by default and return the new dataframe. 
# If selected_name is not NA, then return the dataframe with rows that have any of the colname in the selected_name removed. 
cleanDF = function(df, colname, selected_name = NA){
  filter = !vector(mode = "logical", length = nrow(df))
  if (!is.na(selected_name)) {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !(x %in% selected_name))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  } else {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !is.na(x))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  }
  return(df_clean)
}
```


#### Add missing data of NA from Internet
```{r}
# population missing data get added 
acadia_louisiana = final_data_no_AK$county_name == "acadia " & final_data_no_AK$state_names == "louisiana"
final_data_no_AK[acadia_louisiana, ]$`total population` = 2919
final_data_no_AK[acadia_louisiana, ]$`white population` = 1051
final_data_no_AK[acadia_louisiana, ]$`black population` = 1787
```


### Knn training for 2016 winning party
Data cleaning: add missing data found in internet or get rid of not needed data
```{r}
# following code get ride of some rows because they do not have voting information from 2016 and can't be found online, thus cannot be included in the knn analysis because knn can't take any NA data
final_data_no_AK = final_data_no_AK[c(-3200),]

selected_col = names(final_data_no_AK)[c(1,2,8, 10,12, 14, 19:21, 38, 39)]
df_used = cleanDF(df = final_data_no_AK, colname = selected_col)
```


```{r}
choose16 = c(1,2,8, 10, 19:21, 38, 39)
voting16 = df_used[, choose16]
voting16$vote_percent_gop16 = as.numeric(sub("%", "", voting16$vote_percent_gop16))
voting16$vote_percent_dem16 = as.numeric(sub("%", "", voting16$vote_percent_dem16))
win_gop = voting16$vote_percent_gop16 >= voting16$vote_percent_dem16
voting16$win_gop = factor(x = win_gop, levels = c(TRUE, FALSE), labels = c(1, 2))

choose12 = c(1,2,12, 14, 19:21, 38, 39)
voting12 = df_used[ , choose12]
voting12$vote_percent_gop12 = as.numeric(sub("%", "", voting12$vote_percent_gop12))
voting12$vote_percent_dem12 = as.numeric(sub("%", "", voting12$vote_percent_dem12))
win_gop = voting12$vote_percent_gop12 >= voting12$vote_percent_dem12
voting12$win_gop = factor(x = win_gop, levels = c(TRUE, FALSE), labels = c(1, 2))

```

why do we randomize based on the states not on the country?
Bias sample may pick too many from certain region if certain state has more counties, then in the country level, those counties has a higher probability to be picked.

```{r}
# find the index in df with corresponding "county_name" and "state_names" in selected_df
corresponding_ind = function(selected_df, df, x){
   which((df$county_name==selected_df[x, "county_name"]) &
          (df$state_names==selected_df[x, "state_names"]))[1]
}

```


```{r}
# choose the index for training set and test set
set.seed(24687531)
states = unique(voting12$state_names)
votingTrain_gop_ind_1 = c()
votingTrain_gop_ind_2 = c()
votingTest_gop_ind = c()

for(state in states){
  if(!is.na(state)){
    filter = (!is.na(voting12$state_names) & voting12$state_names == state)
    selected_df = voting12[filter, ]
    nTotal = nrow(selected_df)
    partition = floor(nTotal/3)
    permuteIndices = sample(nTotal)
    if(length(permuteIndices) == 1){
      part3 = permuteIndices[1]
      votingTest_gop_ind = c(votingTest_gop_ind, part3)
    } else if(length(permuteIndices) == 2){
      part1 = permuteIndices[1]
      part2 = permuteIndices[2]
      votingTrain_gop_ind_1 = c(votingTrain_gop_ind_1, part1)
      votingTrain_gop_ind_2 = c(votingTrain_gop_ind_2, part2)
    } else {
      part1 = permuteIndices[1:partition]
      part2 = permuteIndices[(partition+1):(partition+partition)]
      part3 = permuteIndices[(partition+partition+1):(length(permuteIndices))]
      ind1 = unlist(sapply(part1, function(x) corresponding_ind(selected_df, voting12, x)))
      ind2 = unlist(sapply(part2, function(x) corresponding_ind(selected_df, voting12, x)))
      ind3 = unlist(sapply(part3, function(x) corresponding_ind(selected_df, voting12, x)))
      votingTrain_gop_ind_1 = c(votingTrain_gop_ind_1, ind1)
      votingTrain_gop_ind_2 = c(votingTrain_gop_ind_2, ind2)
      votingTest_gop_ind = c(votingTest_gop_ind, ind3)
    }
  }
}

votingTrain_gop_1 = voting12[votingTrain_gop_ind_1,]
votingTrain_gop_2 = voting12[votingTrain_gop_ind_2,]
votingTest_gop = voting12[votingTest_gop_ind,]
```


```{r}
nrow(votingTrain_gop_1)+nrow(votingTrain_gop_2)+nrow(votingTest_gop)
voting12[which(is.na(voting12$state_names)),]
```


nrow(votingTest_gop)+nrow(votingTrain_gop_1)+nrow(votingTrain_gop_2) is one less than nrow(voting12) because one row has county_name = "district of columbia" but NA data everywhere, including the voting data. Thus it is removed.

#### cross validation
Set up the 2 fold matrix
```{r}
v = 2
folds_gop_won = matrix(data = c(votingTrain_gop_ind_1, votingTrain_gop_ind_2), ncol = 2)
```


```{r}
library(class)
training_set_nrow = length(votingTrain_gop_ind_1)
nTain_gop_won = training_set_nrow *2
ks = c(seq(1, 10, by = 0.5))

preds_gop_won = matrix(nrow = nTain_gop_won, ncol = length(ks))

for (i in 1:v) {
  trainFold = as.integer(folds_gop_won[, -i])
  testFold = folds_gop_won[, i]
  
  for (j in 1:length(ks)) {
    start_indx = (i-1)*training_set_nrow+1
    end_indx = i*training_set_nrow
    preds_gop_won[c(start_indx:end_indx),j ] = knn(train = voting12[trainFold, c(3:9)], test = voting12[testFold, c(3:9)], cl = voting12[trainFold,10], k=ks[j])
  }
}
```


Calculate the classification rate for each k values
```{r, error = TRUE}
# find the best prediction
kRates = apply(preds_gop_won, 2, function(oneSet) {
  gop_won = c(as.numeric(votingTrain_gop_1$win_gop), as.numeric(votingTrain_gop_2$win_gop))
  result = sum (oneSet == gop_won) / nTain_gop_won
  return(result)
  }
)
```


### Choose the Value for `K`

From our plot and the following statistics, choose a value for `k`.  You may not want to choose the `k` with the smallest error, but choose a slightly larger `k` that has nearly the same error rate.

```{r, error=TRUE}
library(ggplot2)
ind = which.max(kRates)
ks[ind]

kRes = data.frame(ks, kRates)
ggplot(data = kRes, aes(x = ks, y = kRates)) +
  geom_line() + 
  labs(x = "Complexity Parameter", y = "K value")
```


### Final Assessment of the Knn Predictor

Now that you have selected `k` using cross-validation on your training data, we can 

* Build the predictor with the chosen `k` on all of `votingTrain_gop_1` and `votingTrain_gop_2`

* Predict the winner for the observtions in `votingTest_gop` and `voting16`

* Calculate the classification rate for `votingTest_gop` and `voting16`

```{r, error=TRUE}
kChoice = ks[ind+4]

training_set = rbind(votingTrain_gop_1,votingTrain_gop_2)

# Predict the winner for the observtions in `votingTest_gop` and `voting16`
votingTest_gop_pred = knn(train = training_set[c(3:9)], test = votingTest_gop[, c(3:9)], cl = training_set[,10], k=kChoice)

voting16_gop_pred = knn(train = training_set[c(3:9)], test = voting16[, c(3:9)], cl = training_set[,10], k=kChoice)

# Calculate the classification rate for `votingTest_gop` and `voting16`

votingTest_gop_classRate = sum(votingTest_gop_pred == votingTest_gop$win_gop) / nrow(votingTest_gop)

votingTest_gop_classRate

voting16_gop_classRate = sum(voting16_gop_pred == voting16$win_gop) / nrow(voting16)

voting16_gop_classRate
```

############################
## Cross Validation training for 2016 democrate voting percentage
```{r}
# use final_data_no_AK for predictor base on the voting percent statistic from 12 with population feature. Voting percent statistic from 16 is used for Testing
choose16_dem = c(10, 19:21)
voting16_dem = df_used[choose16_dem]
colnames(voting16_dem)[1] = c('vote_percent')

choose16_gop = c(8,19:21)
voting16_gop = df_used[choose16_gop]
colnames(voting16_gop)[1] = c('vote_percent')

# Only want education statistic
choose12_dem = c(14, 19:21)
voting12_dem = df_used[ , choose12_dem]
colnames(voting12_dem)[1] = c('vote_percent')

choose12_gop = c(12, 19:21)
voting12_gop = df_used[ , choose12_gop]
colnames(voting12_gop)[1] = c('vote_percent')

```

Create a 10-column matrix called `folds` that contains indices for partitioning the `voting12_dem` data frame into 10 folds.

```{r, error=TRUE}
set.seed(24687531)
nTotal = nrow(voting12_dem)
chooseTest = sample(nTotal, size = 457, replace = FALSE)
votingTest_dem = voting12_dem[chooseTest, ]
votingTrain_dem = voting12_dem[ -chooseTest, ]

nTrain = nrow(votingTrain_dem)
# Set the seed so we all get the same results
set.seed(12344321)
permuteIndices = sample(nTrain)

v = 10
folds_dem = matrix(permuteIndices, ncol = v)
```



```{r, error=TRUE}
library(rpart)
cps = c(seq(0.0001, 0.001, by = 0.0001), 
       seq(0.001, 0.01, by = 0.001),
       seq(0.01, 0.1, by = 0.01))
preds_dem = matrix(nrow = nTrain, ncol = length(cps))

for (i in 1:v) {
  trainFold = as.integer(folds_dem[, -i])
  testFold = folds_dem[, i]
  
  for (j in 1:length(cps)) {
    print(c("Enter", i, j))
    tree = rpart(vote_percent ~ .,
            data = votingTrain_dem[trainFold, ], 
            method = "class",
            control = rpart.control(cp = cps[j]))
    preds_dem[testFold,j ] = 
      predict(tree, 
              newdata = votingTrain_dem[testFold, -1],
              type = "class")
  }
}
```


```{r, error = TRUE}
# find the best prediction
cvRates = apply(preds_dem, 2, function(oneSet) {
  dev = as.numeric(votingTrain_dem$vote_percent)
  result = sum (oneSet == dev) / nTrain
  return(result)
  }
)
```


#### Choose the Value for `cp`

From our plot and the following statistics, choose a value for `cp`.  You may not want to choose the `cp` with the smallest error, but choose a slightly larger `cp` that has nearly the same error rate.

```{r, error=TRUE}
library(ggplot2)
ind = which.max(cvRates)

cvRes = data.frame(cps, cvRates)
ggplot(data = cvRes, aes(x = cps, y = cvRates)) +
  geom_line() + 
  labs(x = "Complexity Parameter", y = "Classification Rate")
```


```{r, error=TRUE}
cpChoice = 

finalTree = rpart(vote_percent ~ .,
                  data = votingTrain_dem, 
                  method = "class",
                  control = rpart.control(cp = cpChoice))

# Best model
testPreds = predict(finalTree, 
              newdata = voting16_dem,
              type = "class")

classRate = sum(testPreds == voting16_dem$vote_percent) / 
  nrow(voting16_dem)

classRate
```


## Package used
class
