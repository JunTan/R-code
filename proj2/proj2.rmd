---
title: "E-project"
output: html_document
Authors: Jun Tan (SID:26370084), Peirang Xu (SID:25117973), Ze Gong, Qing Guo, WU Peichen
---

### Step 1: DATA WRANGLING

## Merging dataset1 and dataset2: (Peirang Xu)

```{r}
#2016 Voting Data
require(scales)
mydata1 = read.csv("http://www.stat.berkeley.edu/users/nolan/data/voteProject/2016_US_County_Level_Presidential_Results.csv")
mydata1 = as.data.frame(mydata1)
mydata1$state_names = state.name[match(mydata1$state_abbr, state.abb)]
mydata1 =mydata1[,c("state_names","county_name","votes_gop","per_gop","votes_dem","per_dem")]
colnames(mydata1) = c("state_names","county_name","vote_count_gop16","vote_percent_gop16","vote_count_dem16","vote_percent_dem16")
mydata1$vote_percent_dem16 = percent(mydata1$vote_percent_dem16)
mydata1$vote_percent_gop16 = percent(mydata1$vote_percent_gop16)
mydata1 = data.frame(lapply(mydata1, as.character), stringsAsFactors = FALSE)
mydata1$county_name = gsub("County|city|City","", mydata1$county_name)
mydata1$county_name = sub("  ", " ", mydata1$county_name)

#2012 Voting Data
require(XML)
Names = scan("http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2012/stateNames.txt", character())
state_names = Names[c(-1, -3)]
urls = paste("http://www.stat.berkeley.edu/~nolan/data/voteProject/countyVotes2012/", state_names ,".xml", sep = "")

vtDoc <- list(0)
vtRoot <- list(0)
mydata2 = for(i in 1:length(urls)){
  vtDoc[[i]] = xmlParse(urls[i])
  vtRoot[[i]] = xmlRoot(vtDoc[[i]])
}

getData <- function(x) {
  county_name = xpathSApply(x, '//th[@class = "results-county"]/text()',xmlValue)[-1]
  vote_count_gop12 = xpathSApply(x, '//tr[@class = "party-republican race-winner" or @class = "party-republican"]/td[@class ="results-popular"]',xmlValue)
  vote_percent_gop12 = xpathSApply(x, '//tr[@class = "party-republican race-winner" or @class = "party-republican"]/td[@class = "results-percentage"]',xmlValue) 
  vote_count_dem12 = xpathSApply(x, '//tr[@class = "party-democrat" or @class="party-democrat race-winner"]/td[@class ="results-popular"]',xmlValue)
  vote_percent_dem12 = xpathSApply(x, '//tr[@class = "party-democrat" or @class ="party-democrat race-winner"]/td[@class = "results-percentage"]', xmlValue)
  df_col <- data.frame(county_name, vote_count_gop12, vote_percent_gop12, vote_count_dem12, vote_percent_dem12)
  return(df_col)
}

getData2 = lapply(vtRoot, getData)
rows = sapply(getData2, nrow)
state_names = as.character(rep(state.name, rows))
getData2 = do.call(rbind, getData2)
mydata2 = cbind(state_names, getData2)
mydata2 = data.frame(lapply(mydata2, as.character), stringsAsFactors = FALSE)
mydata2$county_name = sub("  ", " ", mydata2$county_name)

#merge two dataframe
data1 = merge(mydata1, mydata2, by = c("state_names", "county_name"), all = TRUE)

```

When I merge the first two data frames, I firstly imported the two datasets of 2016 voting results and 2012 voting results using “read.csv” and “xmlParse”. Then, I kept the county name, state name, vote counts and percentage of Democrats and Republicans in the two data frames. when I merge the first two data frames, I used state name and county name as “by”, and all = TRUE to keep all the rows in the merge in case for further use in the project, so as a result, there are NAs in the merged data frame. In the merge of these two data frames, for example, I found that in the 2016 voting data, the county name of the state Alaska is not specified as is specified in the 2012 voting data, and in the merged data frame, I kept the voting result for Alaska in both 2012 and 2016. Also, the voting data for Arkansas in 2012 is incomplete, but I kept all the rows in 2016 Arkansas voting data.

## Merging data1 and dataset3: (Qing Guo)

```{r}
require("rio")
#2008 Voting Data
#loading data from excel
path = "http://www.stat.berkeley.edu/users/nolan/data/voteProject/" 
file = paste(path, "countyVotes2008.xlsx",sep="")
total =  import(file)#import the first sheet only
states = total$STATE[1:51]
states = gsub("\\*", "", states) #get rid of the * at the end of some state names
mydata3 = list(total) #coerce total into a list
i=2
for (state in states){
  if (state=="D.C."){
    next
  }
  mydata3[[i]]<-import(file,which=state)#the "which" argument can be used to specify the sheet name
  i=i+1
}
#change column names to be algined with those of data1:
  for (i in 2:51){
  colnames(mydata3[[i]]) = c("county_name", "Total Precincts", "Precincts Reporting", "vote_count_dem08", "vote_count_gop08", "other", "NA")
  }
#add percentage to each dataframe
for (i in 2:51){
    mydata3[[i]]$"vote_percent_gop08" = percent(mydata3[[i]]$vote_count_gop08/(sum(mydata3[[i]]$vote_count_dem08, mydata3[[i]]$vote_count_gop08, mydata3[[i]]$other, na.rm = TRUE)))
    mydata3[[i]]$"vote_percent_dem08" = percent(mydata3[[i]]$vote_count_dem08/(sum(mydata3[[i]]$vote_count_dem08, mydata3[[i]]$vote_count_gop08, mydata3[[i]]$other, na.rm = TRUE)))
    
  }
#add the state column for each county
states = states[states!= "D.C."]
for (i in 1:50){
  mydata3[[i+1]]$"state_names" = states[i]
}
#drop Total Precincts, Precincts Reporting, NAs and Other
for (i in 2:51){
  mydata3[[i]] = mydata3[[i]][ , -c(2,3,6,7)]
}
#drop the first total table
mydata3 = mydata3[-1]
#convert from list to a data frame and clean the dataframe
mydata3 = do.call(rbind, lapply(mydata3, data.frame, stringsAsFactors=FALSE))
mydata3 = mydata3[c("state_names", "county_name", "vote_count_gop08", "vote_percent_gop08", "vote_count_dem08", "vote_percent_dem08")]
mydata3$county_name = sub("  ", " ", mydata3$county_name)
#merge two dataframe
data2 = merge(mydata3, data1, by = c("state_names", "county_name"), all = TRUE)

```

For the excel dataset, the alaska voting data and DC voting data are missing in the excel tabs. In order to be algined with the former dataset, I calculated the percentage of democrats and republicans votes in each county. Before the merge, I did some data cleaning so that my 2008 dataset could be adpated to the previous dataset. Since the first table is about the results of each state and I don't need that for our dataframe, I dropped it. During the merge, I used union instead of intersection because at this point we don't know if counties with NA in 2008 have voting data or not for other election years. So I saved those NAs for further use. For instance, the Alaska data is still missing but I choose to keep those county names first even though it gives me back NAs because we want to wait to see the data of other years.

## Merging data2 and dataset4: (WU, Peichen)

```{r}
#reading election results of 2004 into data frame mydata4 
mydata4 = read.table("http://www.stat.berkeley.edu/~nolan/data/voteProject/countyVotes2004.txt", header = TRUE)
mydata4$countyName = as.character(mydata4$countyName)
mydata4$states = gsub(",[a-z ]*", "", mydata4$countyName)
countyName = gsub("[a-z ]*,", "", mydata4$countyName)
countyName = sapply(countyName, function(x) paste(x, " ", sep = ""))
mydata4$countyName = countyName
mydata4 = mydata4[, c(4, 1, 2, 3)]
names(mydata4) = c("state_names", "county_name", "vote_count_gop04", "vote_count_dem04")
sum = mydata4$vote_count_gop04 + mydata4$vote_count_dem04
mydata4$vote_percent_gop04 = percent(mydata4$vote_count_gop04/sum)
mydata4$vote_percent_dem04 = percent(mydata4$vote_count_dem04/sum)

#merging mydata4 with data2
data2$state_names = tolower(data2$state_names)
data2$county_name = tolower(data2$county_name)
data3 = merge(data2, mydata4, by = c("state_names", "county_name"), all = TRUE)

```
After merging the main data frame with the results of 2004, the number of rows increases from 3459 to 3506. But the election results of 2004 only contains 2975 rows, since it only contains the results of counties form 48 states (no data from Alaska and Hawaii). 

## Merging data3 and dataset5: (Jun Tan)

```{r}
# Read in data
B01003 = read.csv(file="http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/B01003.csv",head=TRUE,sep=",")
B01003 = B01003[c(-1,-2)]
```

The first and second column are eliminated because they are not useful in terms of analyzing the correlation betweeen the voting statistic with other census data.

```{r}
DP02 = read.csv(file="http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP02.csv",head=TRUE,sep=",")
DP02 = DP02[c("GEO.display.label", "HC03_VC86", "HC03_VC87", "HC03_VC88", "HC03_VC89", "HC03_VC90", "HC03_VC91", "HC03_VC93", "HC03_VC94")]

DP03 = read.csv(file="http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP03.csv",head=TRUE,sep=",")
DP03 = DP03[c("GEO.display.label", "HC03_VC50", "HC03_VC51", "HC03_VC52", "HC03_VC54", "HC03_VC56", "HC03_VC58", "HC03_VC59", "HC03_VC62")]
```

DP02 gets out the percentage of education level of the population and DP03 gets out the percentage of the occupation in different area of the population.

```{r}
# Function for extract state and county/city from a vector of string. Return values have the first half as county, the bottom half as state
get_county_state = function(county_states) {
  county_states = sapply(county_states, as.character)
  # Get rid of hex character in the county name
  county_states = gsub("[\x01-\x1f\x7f-\xff]", "", county_states) 
  
  county_states = county_states[!is.na(county_states)]
  county_states = strsplit(county_states, ",")
  county_names = sapply(county_states, function(x) x[1])
  county_names = strsplit(county_names, "County")
  county_names = sapply(county_names, function(x) x[1])
  state_names = sapply(county_states, function(x) x[2])
  state_names = sub(" ", "", state_names)
  return(c(county_names, state_names))
}
```


```{r}
# data process for B01003
# Split county names and state names
split_county_states = get_county_state(B01003$GEO.display.label)
len = length(split_county_states)
B01003$county_name = split_county_states[0:(len/2)]
B01003$state_name = split_county_states[(len/2+1):len]

# Extract the data frame, each county-state pair in a row with population stats
col_names =  c("county_name", "state_name", "POPGROUP.display.label", "HD01_VD01", "HD02_VD01")
B01003_total = B01003[B01003$POPGROUP.id == 1, col_names]
B01003_white = B01003[B01003$POPGROUP.id == 2, col_names]
B01003_black = B01003[B01003$POPGROUP.id == 4, col_names]

# Merge different stats
B01003_total_white = merge(x = B01003_total, y = B01003_white, by.x = c("county_name", "state_name"), by.y = c("county_name", "state_name"), all.x = TRUE)

B01003_clean = merge(x = B01003_total_white, y = B01003_black, by.x = c("county_name", "state_name"), by.y = c("county_name", "state_name"), all.x = TRUE)

# Clean the final dataframe
B01003_clean = B01003_clean[c("county_name", "state_name", "HD01_VD01.x", "HD01_VD01.y", "HD02_VD01")]
colnames(B01003_clean) = c("county_name", "state_name", "total population", "white population", "black population")
B01003_clean$`black population` = as.numeric(B01003_clean$`black population`)
```

The last step subset out the needed information to analysis including total population, white population and black population.

```{r}
# Process DP02
# Filter out county_name and state_name
names = c("county_states", "9-12th grade no diploma(%)", "high school grad(%)", "some college, no deg(%)", "AS(%)", "BS(%)", "MS/PHD(%)", "% high school of higher(%)", "% BS or higher(%)")
colnames(DP02) = names
split_county_states = get_county_state(DP02$county_states)
len = length(split_county_states)
DP02$county_name = split_county_states[0:(len/2)]
DP02$state_name = split_county_states[(len/2+1):len]

# Clean DP02
DP02_clean = DP02[c("county_name", "state_name", names[2:length(names)])]
```


```{r}
# Process DP03
names = c("county_states", 
          "Agriculture, forestry, fishing and hunting, and mining(%)", 
          "Construction", "Manufacturing", "Retail trade", "Information(%)", 
          "Professional, scientific, and management, and administrative and waste management services(%)", 
          "Educational services, and health care and social assistance(%)",
          "Public administration(%)")
colnames(DP03) = names
split_county_states = get_county_state(DP03$county_states)
len = length(split_county_states)
DP03$county_name = split_county_states[0:(len/2)]
DP03$state_name = split_county_states[(len/2+1):len]

# Clean DP03
wanted_coln = names[2:length(names)]
DP03_clean = DP03[c("county_name", "state_name", wanted_coln)]
```

B01003_clean, DP03_clean and DP02_clean are denoted to be dataframes with county_name, state_name and corresponding census information from the dataset.

```{r}
# Merge B01003_clean, DP03_clean and DP02_clean
inter1 = merge(x = B01003_clean, y = DP02_clean, by.x = c("county_name", "state_name"), by.y = c("county_name", "state_name"), all = TRUE)
filter = sapply(inter1$county_name, is.na)
inter1 = inter1[!filter,]
all_3 = merge(x = inter1, y = DP03_clean, by.x = c("county_name", "state_name"), by.y = c("county_name", "state_name"), all = TRUE)
all_3 = all_3[, c(2, 1, 3:21)]
```

The final dataframe "all_3" has all rows from DP03_clean, DP02_clean, and B01003_clean.

```{r}
# Merge my dataframe with the master dataframe
all_3$state_name = tolower(all_3$state_name)
all_3$county_name = tolower(all_3$county_name)
data4 = merge(x = data3, y = all_3, by.x = c("state_names", "county_name"), by.y = c("state_name", "county_name"), all.x = TRUE)
```

data4 includes all information about voting from 2004, 2008, 2012 and 2016 and census data of population, education level and occupation from 2010. We want to keep the rows that has voting statistics because we are analyzing the voting in this project. Those rows with education or occupation information only does not contribute to the analysis of the voting.

## Merging data4 and dataset6: (Gong Ze)

```{r}
#GML data
require(XML)
GMLsource = "http://www.stat.berkeley.edu/users/nolan/data/voteProject/counties.gml"
GMLparse = xmlParse(GMLsource)
GMLroot = xmlRoot(GMLparse)
countyname = xpathSApply(GMLroot, '//county/gml:name',xmlValue)
```


```{r}
state_name = xpathSApply(GMLroot,'//state/gml:name', xmlValue)
state_name_clean = gsub("\n[ ]*", "", state_name)
state_count = sapply(state_name, function(i)
           length(xpathApply(GMLroot, paste0("//state[gml:name='",i,"']/county/gml:name"))))
state_names = rep(state_name_clean, times=state_count)
```


```{r}
latitude = as.numeric(xpathSApply(GMLroot, '//gml:Y', xmlValue))/1000000
longitude = as.numeric(xpathSApply(GMLroot, '//gml:X', xmlValue))/1000000
dataset6 = data.frame(countyname, state_names,latitude,longitude)
dataset6$state_names = tolower(dataset6$state_names)
dataset6$countyname = gsub("county", "", tolower(dataset6$countyname))
dataset6$countyname = gsub("\n[ ]*", "", dataset6$countyname)
big_data = merge(x = data4, y = dataset6, by.x = c("county_name", "state_names"), by.y = c("countyname", "state_names"), all.x = TRUE)
```

## Clean the final dataframe

```{r}
# clean the given dataframe
# remove rows that have any of the column = NA removed by default and return the new dataframe. 
# If selected_name is not NA, then return the dataframe with rows that have any of the colname in the selected_name removed. 
cleanDF = function(df, colname, selected_name = NA){
  filter = !vector(mode = "logical", length = nrow(df))
  if (!is.na(selected_name)) {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !(x %in% selected_name))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  } else {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !is.na(x))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  }
  return(df_clean)
}
```


```{r}
# combine rows with same state name and similar county name because dataset classify county and city differently
comb_similar_row = function(df){
  temp_df = df
  result_df = NA 
  result_df = rbind(result_df, temp_df[1,])
  county_name = temp_df$county_name
  state_names = temp_df$state_names
  for (i in c(2:nrow(temp_df))){
    cName = temp_df[i,]$county_name
    sName = temp_df[i,]$state_names
    filter = (state_names == sName) & grepl(pattern = cName, x = county_name) & !(sName %in% result_df$state_names) & !(cName %in% result_df$county_name)
    p = gsub(pattern = " [A-z]*", " ", c(cName))
    p = gsub(pattern = "  ", " ", p)
    filter2 = grepl(pattern = p[1], x = result_df[result_df$state_names == sName, "county_name"])
    if (!is.na(p) & all(filter2 == FALSE)) {
      result_df = rbind(result_df, temp_df[i,])
    } else if (!is.na(p) & !any(filter2)) {
      similar_row = temp_df[filter, ]
      similar_cNames = similar_row$county_name
      similar_sNames = similar_row$state_names
      for (name in unique(similar_sNames)) {
        filter_df = similar_row[similar_sNames == name, ]
        df1 = filter_df[1,]
        df2 = filter_df[2,]
        df1[which(is.na(df1) & !is.na(df2))] = df2[which(is.na(df1) & !is.na(df2))]
        result_df = rbind(result_df, df1)
      }
    }
  }
  return(result_df[2:nrow(result_df), ])
}
```


```{r}
# clean the big_data by combining the rows with same state_name and similar county/city name
final_data = comb_similar_row(big_data)
```

There are about 200 rows reduction after combining similar rows because some cites were recorded as its city name in some years in some dataset but also recorded as the county it belongs to in some other years in some other datasets. That results in some data are missing in some row but actually those data are in the row paired with the city name or county name. Thus the 'comb_similar_row' merges those missing data together with the corresponding city/county name.

```{r}
# pick rows with state_names are not alaska because alaska does not have sufficient information needed for analysis
final_data_no_AK = cleanDF(final_data, colname = c("state_names"), selected_name = c("alaska"))
save(final_data_no_AK,file="final_data_no_AK.rda")
```

Because most of data from Alaska are missing, including the voting from all different years, Alaska won't contribute a lot to our predictions and plots, it can be removed without causing significant impact.


## Step2



## Step 3 Map (Peirang Xu)
```{r}
#clean up data for plotting
all_counties = final_data_no_AL[complete.cases(final_data_no_AL[,38:39]),]
all_counties = final_data_no_AL[complete.cases(final_data_no_AL[,c(3,5)]),]
all_counties = all_counties[all_counties$state_names != "hawaii",]
```


```{r}
#plot election results for 2004 election
library(ggplot2)
library(maps)

#load us map data
all_states <- map_data("state")

#plot all states with ggplot
p = ggplot() + 
   geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour="black", fill= NA )+ 
  coord_fixed(1.3)

#plot county boundaries
counties = map_data("county")
p = p + geom_polygon(data = counties, aes(x= long, y= lat, group = group), color = "grey", fill = "white")
#add in data

#as.numeric(all_counties$vote_count_gop04)
#as.numeric(all_counties$vote_count_dem04)

data04 = data.frame(all_counties$vote_count_gop04, all_counties$vote_count_dem04)
data04.max = apply(data04, 1, max)
party04 = sapply(1:nrow(data04), function(x) 
  colnames(data04)[grep(data04.max[x], data04[x,])])
gop04 = grepl("all_counties.vote_count_gop04" , party04)
dem04 = grepl("all_counties.vote_count_dem04" , party04)
gop04[gop04] = 1
dem04[dem04] = 2
all_counties$party04 = factor(gop04+dem04, levels = c(0,1,2), labels = c("tie", "gop", "dem"))

#plot data for 2004 election
p = p +
  geom_point(aes(x = all_counties$longitude, y = all_counties$latitude,color = all_counties$party04, alpha = 0.2, size = all_counties$vote_count_gop04 + all_counties$vote_count_dem04)) + scale_size(name ="vote count", range = c(0,15))+ scale_color_manual(name = "leading patry",values = c( "gop" = "red", "dem" = "blue", "tie" = "green")) + labs(x = "longitude", y = "latitude", title = "county voting results for 2004 election")
p

```


```{r}
#plot election results for 2008 election
library(ggplot2)
library(maps)

#load us map data
all_states <- map_data("state")

#plot all states with ggplot
p = ggplot() + 
   geom_polygon( data=all_states, aes(x=long, y=lat, group = group),colour="black", fill= NA )+ 
  coord_fixed(1.3)

#plot county boundaries
counties = map_data("county")
p = p + geom_polygon(data = counties, aes(x= long, y= lat, group = group), color = "grey", fill = "white")

#add in data
#as.numeric(all_counties$vote_count_gop08)
#as.numeric(all_counties$vote_count_dem08)

data08 = data.frame(all_counties$vote_count_gop08, all_counties$vote_count_dem08)
data08.max = apply(data08, 1, max)
party08 = sapply(1:nrow(data08), function(x) 
  colnames(data08)[grep(data08.max[x], data08[x,])])
gop08 = grepl("all_counties.vote_count_gop08" , party08)
dem08 = grepl("all_counties.vote_count_dem08" , party08)
gop08[gop08] = 1
dem08[dem08] = 2
all_counties$party08 = factor(gop08+dem08, levels = c(0,1,2), labels = c("tie", "gop", "dem"))

#plot data for 2008 election
p = p +
  geom_point(aes(x = all_counties$longitude, y = all_counties$latitude,color = all_counties$party08, alpha = 0.2, size = all_counties$vote_count_gop08 + all_counties$vote_count_dem08))+ scale_size(name ="vote count", range = c(0,15))+ scale_color_manual(name = "leading patry",values = c( "gop" = "red", "dem" = "blue", "tie" = "green")) + labs(x = "longitude", y = "latitude", title = "county voting results for 2008 election")
p
```


## Step 4: MODELING (Jun Tan, PEICHEN WU)
```{r}
# load data
load("final_data_no_AK.Rda")
```


## Predictor for the change from 2012 to 2016
## Add missing data of NA from Internet
```{r}
load("final_data_no_AK.Rda")
# population missing data get added 
acadia_louisiana = final_data_no_AK$county_name == "acadia " & final_data_no_AK$state_names == "louisiana"
final_data_no_AK[acadia_louisiana, ]$`total population` = 2919
final_data_no_AK[acadia_louisiana, ]$`white population` = 1051
final_data_no_AK[acadia_louisiana, ]$`black population` = 1787

df_used = final_data_no_AK
```


## Loading Data
```{r}
library(rpart)
library(rpart.plot)
library(ggplot2)
```

## factoring results in each county
```{r}
## results for 2008

sum(df_used$vote_count_dem08 == df_used$vote_count_gop08, na.rm = TRUE) == 0
# The statement above can tell us id there is any draw in the counties. We got False for this statement, and the sum is 3. So there're three counties in which there is a draw between the two parties.
# To keep our analysis clean and consistent we will drop the three counties in which there is a tie by assigning NA to their result08 variable. All three counties have total vote counts less than 1,000 so dropping them will not have a significant effect on our results.
df_used$result08 = numeric(nrow(df_used))
for (i in 1:nrow(df_used)) {
  if (!is.na(df_used$vote_count_dem08[i])) {
    if (df_used$vote_count_dem08[i] > df_used$vote_count_gop08[i]) {
      df_used$result08[i] = 1
    } else if (df_used$vote_count_dem08[i] < df_used$vote_count_gop08[i]) {
      df_used$result08[i] = 2
    } else {
      df_used$result08[i] = NA
    }
  } else {
    df_used$result08[i] = NA
  }
}
# We factorize the numeric vector. 'dem' means a democrat victory and 'gop' means a republican victory.
df_used$result08 = factor(df_used$result08, labels = c("dem", "gop"))


## results for 2012
sum(df_used$vote_count_dem12 == df_used$vote_count_gop12, na.rm = TRUE) == 0
# The comand above gives us TRUE, so we can be sure that there is no draw in any county.
df_used$result12 = as.numeric(df_used$vote_count_dem12 < df_used$vote_count_gop12) 
df_used$result12 = factor(df_used$result12, labels = c("dem", "gop"))


## results for 2016
sum(df_used$vote_count_dem16 == df_used$vote_count_gop16, na.rm = TRUE) == 0
# The comand above gives us TRUE, so we can be sure that there is no draw in any county.
df_used$result16 = as.numeric(df_used$vote_count_dem16 < df_used$vote_count_gop16) 
df_used$result16 = factor(df_used$result16, labels = c("dem", "gop"))
```


## factoring the change in each county
```{r}
## For changes from 2008 to 2012
df_used$change08_12 = numeric(nrow(df_used))
for (i in 1:nrow(df_used)) {
  if (!is.na(df_used$result08[i]) & !is.na(df_used$result12[i])) {
    if (df_used$result08[i] == "gop" & df_used$result12[i] == "gop") {
      df_used$change08_12[i] = 1
    } else if (df_used$result08[i] == "gop" & df_used$result12[i] == "dem") {
      df_used$change08_12[i] = 2
    } else if (df_used$result08[i] == "dem" & df_used$result12[i] == "gop") {
      df_used$change08_12[i] = 3
    } else {
      df_used$change08_12[i] = 4
    }
  } else {
    df_used$change08_12[i] = NA
  }
}
# factoring 
df_used$change08_12 = factor(df_used$change08_12, labels = c("gop to gop", "gop to dem", "dem to gop", "dem to dem"))


## For changes from 2012 to 2016
df_used$change12_16 = numeric(nrow(df_used))
for (i in 1:nrow(df_used)) {
  if (!is.na(df_used$result12[i]) & !is.na(df_used$result16[i])) {
    if (df_used$result12[i] == "gop" & df_used$result16[i] == "gop") {
      df_used$change12_16[i] = 1
    } else if (df_used$result12[i] == "gop" & df_used$result16[i] == "dem") {
      df_used$change12_16[i] = 2
    } else if (df_used$result12[i] == "dem" & df_used$result16[i] == "gop") {
      df_used$change12_16[i] = 3
    } else {
      df_used$change12_16[i] = 4
    } 
  } else {
    df_used$change12_16[i] = NA
  }
}
# factoring 
df_used$change12_16 = factor(df_used$change12_16, labels = c("gop to gop", "gop to dem", "dem to gop", "dem to dem"))
```

## Modeling
```{r}
## We only want the change variable and education statistics

## Data frame for change from 2008 to 2012
chooseChange08_12 = c(43, 22:29)
Change08_12 = df_used[ , chooseChange08_12]
colnames(Change08_12)[1] = c('change_type')

## Data frame for change from 2012 to 2016
chooseChange12_16 = c(44, 22:29)
Change12_16 = df_used[ , chooseChange12_16]
colnames(Change12_16)[1] = c('change_type')
```

Create a 5-column matrix called `folds` that contains indices for partitioning the `Change08_12` data frame into a training data frame with 5 folds.

```{r, error=TRUE}
## divide "Change08_12" into a training set and a test set
set.seed(24687531)
nTotal = nrow(Change08_12)
chooseTest = sample(nTotal, size = 457, replace = FALSE)
changeTest_dem = Change08_12[chooseTest, ]
changeTrain_dem = Change08_12[ -chooseTest, ]

## Partition the training set into 5 folds for cross validation
nTrain = nrow(changeTrain_dem)
set.seed(12344321)
permuteIndices = sample(nTrain)
v = 5
folds1 = matrix(permuteIndices, ncol = v)
```


## Building the Tree 
```{r, error=TRUE}
cps = c(seq(0.0001, 0.001, by = 0.0001), 
       seq(0.001, 0.01, by = 0.001),
       seq(0.01, 0.1, by = 0.01))
preds1 = matrix(nrow = nTrain, ncol = length(cps))

for (i in 1:v) {
  trainFold = as.integer(folds1[, -i])
  testFold = folds1[, i]
  
  for (j in 1:length(cps)) {
    tree = rpart(change_type ~ .,
            data = changeTrain_dem[trainFold, ], 
            method = "class",
            control = rpart.control(cp = cps[j]))
    preds1[testFold,j ] = 
      predict(tree, 
              newdata = changeTrain_dem[testFold, -1],
              type = "class")
  }
}
```


```{r, error = TRUE}
# find the best prediction
cvRates = apply(preds1, 2, function(oneSet) {
  dev = as.numeric(changeTrain_dem$change_type)
  result = length(oneSet[oneSet == dev]) / nTrain
  return(result)
  }
)
```


#### Choose the Value for `cp`

From our plot and the following statistics, choose a value for `cp`.  You may not want to choose the `cp` with the smallest error, but choose a slightly larger `cp` that has nearly the same error rate.

```{r, error=TRUE}
library(ggplot2)
ind = which.max(cvRates)

cvRes = data.frame(cps, cvRates)
ggplot(data = cvRes, aes(x = cps, y = cvRates)) +
  geom_line() + 
  labs(x = "Complexity Parameter", y = "Classification Rate")
```


## Assess the Classification Tree Predictor with the Test set
```{r, error=TRUE}
cpChoice1 = cps[10]

finalTree1 = rpart(change_type ~ .,
                  data = changeTrain_dem, 
                  method = "class",
                  control = rpart.control(cp = cpChoice1))

# Best model
testPreds1 = predict(finalTree1, 
              newdata = changeTest_dem,
              type = "class")

classRate1 = sum(testPreds1 == changeTest_dem$change_type, na.rm = T)/nrow(changeTest_dem)

classRate1
```


## Assess the Classification Tree Predictor with the change from 2012 to 2016
```{r, error=TRUE}
cpChoice2 = cps[5]

finalTree2 = rpart(change_type ~ .,
                  data = Change08_12, 
                  method = "class",
                  control = rpart.control(cp = cpChoice2))

# Best model
testPreds2 = predict(finalTree2, 
              newdata = Change12_16[,-1],
              type = "class")

classRate2 = sum(testPreds2 == Change12_16$change_type, na.rm = T)/nrow(Change12_16)

classRate2

prp(finalTree2, extra =2)

# Plotting show that the changing in prediction and in the actual data the same
pred_df = data.frame(testPreds2)
pred_plot = ggplot(data = pred_df, aes(pred_df$testPreds2)) + geom_bar()
pred_plot

actual_change_df = data.frame(Change12_16$change_type)
acutal_plot = ggplot(data = actual_change_df, aes(actual_change_df$Change12_16.change_type)) + geom_bar()
acutal_plot
```


```{r}
# clean the given dataframe
# remove rows that have any of the column = NA removed by default and return the new dataframe. 
# If selected_name is not NA, then return the dataframe with rows that have any of the colname in the selected_name removed. 
cleanDF = function(df, colname, selected_name = NA){
  filter = !vector(mode = "logical", length = nrow(df))
  if (!is.na(selected_name)) {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !(x %in% selected_name))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  } else {
    for (name in colname){
      inter_filter = sapply(df[name], function(x) !is.na(x))
      filter = filter & inter_filter
    }
    df_clean = df[filter,]
  }
  return(df_clean)
}
```



## Knn training for 2016 winning party
Data cleaning: add missing data found in internet or get rid of not needed data
```{r}
# following code get ride of some rows because they do not have voting information from 2016 and can't be found online, thus cannot be included in the knn analysis because knn can't take any NA data
final_data_no_AK = final_data_no_AK[c(-3200),]

selected_col = names(final_data_no_AK)[c(1,2,8, 10,12, 14, 19:21, 38, 39)]
df_used = cleanDF(df = final_data_no_AK, colname = selected_col)
```


```{r}
choose16 = c(1,2,8, 10, 19:21, 38, 39)
voting16 = df_used[, choose16]
voting16$vote_percent_gop16 = as.numeric(sub("%", "", voting16$vote_percent_gop16))
voting16$vote_percent_dem16 = as.numeric(sub("%", "", voting16$vote_percent_dem16))
win_gop = voting16$vote_percent_gop16 >= voting16$vote_percent_dem16
voting16$win_gop = factor(x = win_gop, levels = c(TRUE, FALSE), labels = c(1, 2))

choose12 = c(1,2,12, 14, 19:21, 38, 39)
voting12 = df_used[ , choose12]
voting12$vote_percent_gop12 = as.numeric(sub("%", "", voting12$vote_percent_gop12))
voting12$vote_percent_dem12 = as.numeric(sub("%", "", voting12$vote_percent_dem12))
win_gop = voting12$vote_percent_gop12 >= voting12$vote_percent_dem12
voting12$win_gop = factor(x = win_gop, levels = c(TRUE, FALSE), labels = c(1, 2))

```

why do we randomize based on the states not on the country?
Bias sample may pick too many from certain region if certain state has more counties, then in the country level, those counties has a higher probability to be picked.

```{r}
# find the index in df with corresponding "county_name" and "state_names" in selected_df
corresponding_ind = function(selected_df, df, x){
   which((df$county_name==selected_df[x, "county_name"]) &
          (df$state_names==selected_df[x, "state_names"]))[1]
}

```


```{r}
# choose the index for training set and test set
set.seed(24687531)
states = unique(voting12$state_names)
votingTrain_gop_ind_1 = c()
votingTrain_gop_ind_2 = c()
votingTest_gop_ind = c()

for(state in states){
  if(!is.na(state)){
    filter = (!is.na(voting12$state_names) & voting12$state_names == state)
    selected_df = voting12[filter, ]
    nTotal = nrow(selected_df)
    partition = floor(nTotal/3)
    permuteIndices = sample(nTotal)
    if(length(permuteIndices) == 1){
      part3 = permuteIndices[1]
      votingTest_gop_ind = c(votingTest_gop_ind, part3)
    } else if(length(permuteIndices) == 2){
      part1 = permuteIndices[1]
      part2 = permuteIndices[2]
      votingTrain_gop_ind_1 = c(votingTrain_gop_ind_1, part1)
      votingTrain_gop_ind_2 = c(votingTrain_gop_ind_2, part2)
    } else {
      part1 = permuteIndices[1:partition]
      part2 = permuteIndices[(partition+1):(partition+partition)]
      part3 = permuteIndices[(partition+partition+1):(length(permuteIndices))]
      ind1 = unlist(sapply(part1, function(x) corresponding_ind(selected_df, voting12, x)))
      ind2 = unlist(sapply(part2, function(x) corresponding_ind(selected_df, voting12, x)))
      ind3 = unlist(sapply(part3, function(x) corresponding_ind(selected_df, voting12, x)))
      votingTrain_gop_ind_1 = c(votingTrain_gop_ind_1, ind1)
      votingTrain_gop_ind_2 = c(votingTrain_gop_ind_2, ind2)
      votingTest_gop_ind = c(votingTest_gop_ind, ind3)
    }
  }
}

votingTrain_gop_1 = voting12[votingTrain_gop_ind_1,]
votingTrain_gop_2 = voting12[votingTrain_gop_ind_2,]
votingTest_gop = voting12[votingTest_gop_ind,]
```


```{r}
nrow(votingTrain_gop_1)+nrow(votingTrain_gop_2)+nrow(votingTest_gop)
```


nrow(votingTest_gop)+nrow(votingTrain_gop_1)+nrow(votingTrain_gop_2) is one less than nrow(voting12) because one row has county_name = "district of columbia" but NA data everywhere, including the voting data. Thus it is removed.

#### cross validation
Set up the 2 fold matrix
```{r}
v = 2
folds_gop_won = matrix(data = c(votingTrain_gop_ind_1, votingTrain_gop_ind_2), ncol = 2)
```


```{r}
library(class)
training_set_nrow = length(votingTrain_gop_ind_1)
nTain_gop_won = training_set_nrow *2
ks = c(seq(1, 10, by = 0.5))

preds_gop_won = matrix(nrow = nTain_gop_won, ncol = length(ks))

for (i in 1:v) {
  trainFold = as.integer(folds_gop_won[, -i])
  testFold = folds_gop_won[, i]
  
  for (j in 1:length(ks)) {
    start_indx = (i-1)*training_set_nrow+1
    end_indx = i*training_set_nrow
    preds_gop_won[c(start_indx:end_indx),j ] = knn(train = voting12[trainFold, c(3:9)], test = voting12[testFold, c(3:9)], cl = voting12[trainFold,10], k=ks[j])
  }
}
```


Calculate the classification rate for each k values
```{r, error = TRUE}
# find the best prediction
kRates = apply(preds_gop_won, 2, function(oneSet) {
  gop_won = c(as.numeric(votingTrain_gop_1$win_gop), as.numeric(votingTrain_gop_2$win_gop))
  result = sum (oneSet == gop_won) / nTain_gop_won
  return(result)
  }
)
```


### Choose the Value for `K`

From our plot and the following statistics, choose a value for `k`.  You may not want to choose the `k` with the smallest error, but choose a slightly larger `k` that has nearly the same error rate.

```{r, error=TRUE}
library(ggplot2)
ind = which.max(kRates)
ks[ind]

kRes = data.frame(ks, kRates)
ggplot(data = kRes, aes(x = ks, y = kRates)) +
  geom_line() + 
  labs(x = "Complexity Parameter", y = "K value")
```


### Final Assessment of the Knn Predictor

Now that you have selected `k` using cross-validation on your training data, we can 

* Build the predictor with the chosen `k` on all of `votingTrain_gop_1` and `votingTrain_gop_2`

* Predict the winner for the observtions in `votingTest_gop` and `voting16`

* Calculate the classification rate for `votingTest_gop` and `voting16`

```{r, error=TRUE}
kChoice = ks[ind]

training_set = rbind(votingTrain_gop_1,votingTrain_gop_2)

# Predict the winner for the observtions in `votingTest_gop` and `voting16`
votingTest_gop_pred = knn(train = training_set[, c(3:9)], test = votingTest_gop[, c(3:9)], cl = training_set[,10], k=kChoice)


voting16_gop_pred = knn(train = training_set[,c(3:9)], test = voting16[, c(3:9)], cl = training_set[,10], k=kChoice)

# Calculate the classification rate for `votingTest_gop` and `voting16`

votingTest_gop_classRate = sum(votingTest_gop_pred == votingTest_gop$win_gop) / nrow(votingTest_gop)

votingTest_gop_classRate

voting16_gop_classRate = sum(voting16_gop_pred == voting16$win_gop) / nrow(voting16)

voting16_gop_classRate
```


```{r}
result = as.data.frame(table(voting16_gop_pred))
predict_gop_win = result[result$voting16_gop_pred == 1, "Freq"] > result[result$voting16_gop_pred == 2, "Freq"]

actual = as.data.frame(table(voting16$win_gop))
actual_gop_win = actual[actual$Var1 == 1, "Freq"] > actual[actual$Var1 == 2, "Freq"]

output = paste("Prediction for gop is the winner in 2016:",predict_gop_win, ", actually gop is the winner in 2016:",actual_gop_win)

print(output)
```

In the knn modeling section, due to knn can't train on NA data and there are a lot of missing values in different variables that cannot be replaced or found in the internet, it leaves us no choice but to eliminate all counties that has NA data in any variables used for modeling


## References
"http://www.stat.berkeley.edu/users/nolan/data/voteProject/2016_US_County_Level_Presidential_Results.csv"

"http://www.stat.berkeley.edu/users/nolan/data/voteProject/countyVotes2012/stateNames.txt"

"http://www.stat.berkeley.edu/users/nolan/data/voteProject/" 

"http://www.stat.berkeley.edu/~nolan/data/voteProject/countyVotes2004.txt"

"http://www.stat.berkeley.edu/users/nolan/data/voteProject/counties.gml"

"http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/B01003.csv"

"http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP02.csv"

"http://www.stat.berkeley.edu/~nolan/data/voteProject/census2010/DP03.csv"


## Package used
scales
XML
rio

## System info
```{r}
sessionInfo()
```
